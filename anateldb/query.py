# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/queries.ipynb (unless otherwise specified).

__all__ = ['optimize_floats', 'optimize_ints', 'optimize_objects', 'df_optimize', 'connect_db', 'update_radcom',
           'update_stel', 'update_mosaico', 'update_base', 'read_stel', 'read_radcom', 'read_mosaico', 'read_base']

# Cell
import requests
from decimal import *
from typing import *
from gazpacho import Soup
from rich.progress import track
from pathlib import Path
from unidecode import unidecode
import pandas as pd
import pandas_read_xml as pdx
import pyodbc
import collections
from fastcore.utils import listify
from fastcore.foundation import L
from fastcore.test import *
from .constants import *
from pyarrow import ArrowInvalid
getcontext().prec = 5

# Cell
def optimize_floats(df: pd.DataFrame, exclude = None) -> pd.DataFrame:
    floats = df.select_dtypes(include=["float64"]).columns.tolist()
    floats = [c for c in floats if c not in listify(exclude)]
    df[floats] = df[floats].apply(pd.to_numeric, downcast="float")
    return df


def optimize_ints(df: pd.DataFrame, exclude=None) -> pd.DataFrame:
    ints = df.select_dtypes(include=["int64"]).columns.tolist()
    ints = [c for c in ints if c not in listify(exclude)]
    df[ints] = df[ints].apply(pd.to_numeric, downcast="integer")
    return df


def optimize_objects(df: pd.DataFrame, datetime_features: List[str], exclude=None) -> pd.DataFrame:
    for col in df.select_dtypes(include=["object"]).columns.tolist():
        if col not in datetime_features:
            if col in listify(exclude): continue
            num_unique_values = len(df[col].unique())
            num_total_values = len(df[col])
            if float(num_unique_values) / num_total_values < 0.5:
                dtype = "category"
            else:
                dtype = "string"
            df[col] = df[col].astype(dtype)
        else:
            df[col] = pd.to_datetime(df[col]).dt.date
    return df


def df_optimize(df: pd.DataFrame, datetime_features: List[str] = [], exclude = None):
    return optimize_floats(optimize_ints(optimize_objects(df, datetime_features, exclude), exclude), exclude)

# Cell
def connect_db():
    """Conecta ao Banco ANATELBDRO01 e retorna o 'cursor' (iterador) do Banco pronto para fazer iterações"""
    conn = pyodbc.connect(
        "Driver={ODBC Driver 17 for SQL Server};"
        "Server=ANATELBDRO01;"
        "Database=SITARWEB;"
        "Trusted_Connection=yes;"
        "MultipleActiveResultSets=True;",
        timeout=TIMEOUT,
    )
    return conn

# Internal Cell
def row2dict(row):
    """Receives a json row and return the dictionary from it"""
    return {k: v for k, v in row.items()}


def dict2cols(df, reject=()):
    """Recebe um dataframe com dicionários nas células e extrai os dicionários como colunas
    Opcionalmente ignora e exclue as colunas em reject
    """
    for column in df.columns:
        if column in reject:
            df.drop(column, axis=1, inplace=True)
            continue
        if type(df[column].iloc[0]) == collections.OrderedDict:
            try:
                new_df = pd.DataFrame(df[column].apply(row2dict).tolist())
                df = pd.concat([df, new_df], axis=1)
                df.drop(column, axis=1, inplace=True)
            except AttributeError:
                continue
    return df


def parse_plano_basico(row, cols=COL_PB):
    """Receives a json row and filter the column in `cols`"""
    return {k: row[k] for k in cols}


def scrape_dataframe(id_list):
    df = pd.DataFrame()
    for id_ in track(id_list, description="Baixando informações complementares da Web"):
        html = requests.get(ESTACAO.format(id_))
        df = df.append(pd.read_html(Soup(html.text).find("table").html)[0])

    df.rename(columns={'NumFistel': 'Fistel',
                       'Num Serviço': 'Num_Serviço'}, inplace=True)
    return df[["Status", "Entidade", "Fistel", "Frequência", "Classe", 'Num_Serviço', 'Município', 'UF']]

# Internal Cell
def input_coordenates(df, pasta):
    """Input the NA's in Coordinates with those of the cities"""
    municipios = Path(f"{pasta}/municípios.fth")
    if not municipios.exists():
        municipios = Path(f"{pasta}/municípios.xlsx")
        if not municipios.exists():
            raise FileNotFoundError(f"É necessario a tabela de municípios municípios.fth | municípios.xlsx na pasta {pasta}")
        m = pd.read_excel(municipios, engine='openpyxl')
    else:
        m = pd.read_feather(municipios)
    m.loc[
        m.Município == "Sant'Ana do Livramento", "Município"
    ] = "Santana do Livramento"
    m["Município"] = (
        m.Município.apply(unidecode).str.lower().str.replace("'", " ")
    )
    m["UF"] = m.UF.str.lower()
    df["Coordenadas_do_Município"] = False
    df["Latitude"] = df.Latitude.str.replace(",", ".")
    df["Longitude"] = df.Longitude.str.replace(",", ".")
    df.loc[df["Município"] == "Poxoréo", "Município"] = "Poxoréu"
    df.loc[df["Município"] == "Couto de Magalhães", "Município"] = "Couto Magalhães"
    df['Município'] = df.Município.astype('string')
    criteria = ((df.Latitude == "") | (df.Latitude.isna()) | (df.Longitude == '') | (df.Longitude.isna())) & df.Município.isna()
    df = df[~criteria]
    for row in df[((df.Latitude == "") | (df.Latitude.isna()) | (df.Longitude == '') | (df.Longitude.isna()))].itertuples():
        try:
            left = unidecode(row.Município).lower()
            m_coord = (
                m.loc[
                    (m.Município == left) & (m.UF == row.UF.lower()),
                    ["Latitude", "Longitude"],
                ]
                .values.flatten()
                .tolist()
            )
            df.loc[row.Index, "Latitude"] = m_coord[0]
            df.loc[row.Index, "Longitude"] = m_coord[1]
            df.loc[row.Index, "Coordenadas_do_Município"] = True
        except ValueError:
            print(left, row.UF, m_coord)
            continue
    return df

def clean_merge(pasta, df):
    df = df.copy()
    COLS = [c for c in df.columns if "_x" in c]
    for col in COLS:
        col_x = col
        col_y = col.split("_")[0] + "_y"
        if df[col_x].count() > df[col_y].count():
            a, b = col_x, col_y
        else:
            a, b = col_y, col_x

        df.loc[df[a].isna(), a] = df.loc[df[a].isna(), b]
        df.drop(b, axis=1, inplace=True)
        df.rename({a: a[:-2]}, axis=1, inplace=True)

    df.loc[df.Latitude_Transmissor == "", "Latitude_Transmissor"] = df.loc[
        df.Latitude_Transmissor == "", "Latitude_Estação"
    ]
    df.loc[df.Longitude_Transmissor == "", "Longitude_Transmissor"] = df.loc[
        df.Longitude_Transmissor == "", "Longitude_Estação"
    ]
    df.loc[df.Latitude_Transmissor.isna(), "Latitude_Transmissor"] = df.loc[
        df.Latitude_Transmissor.isna(), "Latitude_Estação"
    ]
    df.loc[df.Longitude_Transmissor.isna(), "Longitude_Transmissor"] = df.loc[
        df.Longitude_Transmissor.isna(), "Longitude_Estação"
    ]
    df.drop(["Latitude_Estação", "Longitude_Estação"], axis=1, inplace=True)
    df.rename(
        columns={
            "Latitude_Transmissor": "Latitude",
            "Longitude_Transmissor": "Longitude",
        },
        inplace=True,
    )

    df = input_coordenates(df, pasta)

    df["Frequência"] = df.Frequência.str.replace(",", ".")

    freq_nans = df[df.Frequência.isna()].Id.tolist()
    if freq_nans:
        complement_df = scrape_dataframe(freq_nans)
        df.loc[
            df.Frequência.isna(), ["Status", "Entidade", "Fistel", "Frequência", "Classe",
                                   'Num_Serviço', 'Município', 'UF']
            ] = complement_df.values

    for r in df[(df.Entidade.isna()) | (df.Entidade == '')].itertuples():
        df.loc[r.Index, 'Entidade'] = ENTIDADES.get(r.Fistel, '')

    df.loc[df["Número_da_Estação"] == "", "Número_da_Estação"] = -1
    df["Latitude"] = df["Latitude"].astype("float")
    df["Longitude"] = df["Longitude"].astype("float")
    df["Frequência"] = df.Frequência.astype("float")
    df.loc[df.Serviço == 'OM', 'Frequência'] = df.loc[df.Serviço == 'OM', 'Frequência'].apply(lambda x: Decimal(x) / Decimal(1000))
    df["Frequência"] = df.Frequência.astype("float")
    df['Validade_RF'] = df.Validade_RF.astype('string').str.slice(0,10)
    df.loc[df['Num_Ato'] == '', 'Num_Ato'] = -1
    df['Num_Ato'] = df.Num_Ato.astype('string')
    df['Num_Serviço'] = df.Num_Serviço.astype('category')
    return df_optimize(df, exclude=['Frequência'])

# Internal Cell
def read_estações(path):
    def extrair_ato(row):
        if not isinstance(row, str):
            row = listify(row)[::-1]
            for d in row:
                if not isinstance(d, dict):
                    continue
                if (d.get("@TipoDocumento") == "Ato") and (
                    d.get("@Razao") == "Autoriza o Uso de Radiofrequência"
                ):
                    return d["@NumDocumento"], d["@DataDOU"][:10]
            else:
                return "", ""
        return "", ""

    es = pdx.read_xml(path, ["estacao_rd"])
    dfs = []
    for i in range(es.shape[0]):
        df = pd.DataFrame(es["row"][i]).replace("", pd.NA)
        df = dict2cols(df)
        df.columns = [unidecode(c).lower().replace("@", "") for c in df.columns]
        dfs.append(df)
    df = pd.concat(dfs)
    df = df[df.state.str.contains("-C1$|-C2$|-C3$|-C4$|-C7|-C98$")].reset_index(drop=True)
    docs = L(df.historico_documentos.apply(extrair_ato).tolist())
    df = df.loc[:, COL_ESTACOES]
    df["Num_Ato"] = docs.itemgot(0).map(str)
    df["Data_Ato"] = docs.itemgot(1).map(str)
    df.columns = NEW_ESTACOES
    df['Validade_RF'] = df.Validade_RF.astype('string').str.slice(0,10)
    df["Data_Ato"] = df.Data_Ato.str.slice(0,10)
    for c in df.columns:
        df.loc[df[c] == '', c] = pd.NA
    return df


def read_plano_basico(path):
    pb = pdx.read_xml(path, ["plano_basico"])
    dfs = []
    for i in range(pb.shape[0]):
        df = pd.DataFrame(pb["row"][i]).replace("", pd.NA)
        df = dict2cols(df)
        df.columns = [unidecode(c).lower().replace("@", "") for c in df.columns]
        dfs.append(df)
    df = pd.concat(dfs)
    df = df.loc[df.pais == "BRA", COL_PB].reset_index(drop=True)
    for c in df.columns:
        df.loc[df[c] == '', c] = pd.NA
    df.columns = NEW_PB
    df.sort_values(["Id", "Canal"], inplace=True)
    ENTIDADES.update({r.Fistel : r.Entidade for r in df.itertuples() if str(r.Entidade) == '<NA>'})
    df = df.groupby("Id", as_index=False).first()  # remove duplicated with NaNs
    df.dropna(subset=['Status'], inplace=True)
    df = df[df.Status.str.contains("-C1$|-C2$|-C3$|-C4$|-C7|-C98$")].reset_index(drop=True)
    return df

# Cell
def update_radcom(pasta):
    """Atualiza a tabela local retornada pela query `RADCOM`"""
    with console.status(
        "[cyan]Lendo o Banco de Dados de Radcom...", spinner="earth"
    ) as status:
        try:
            conn = connect_db()
            df = pd.read_sql_query(RADCOM, conn)
            df['Unidade'] = 'MHz'
            df = df_optimize(df, exclude=['Frequência'])
            try:
                df.to_feather(f"{pasta}/radcom.fth")
            except ArrowInvalid:
                Path(f"{pasta}/radcom.fth").unlink()
                df.to_excel(f"{pasta}/radcom.xlsx", engine='openpyxl', index=False)
        except pyodbc.OperationalError:
            status.console.log(
                "Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!"
            )


def update_stel(pasta):
    """Atualiza a tabela local retornada pela query `STEL`"""
    with console.status(
        "[magenta]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...",
        spinner="moon",
    ) as status:
        try:
            conn = connect_db()
            df = pd.read_sql_query(STEL, conn)
            df['Validade_RF'] = df.Validade_RF.astype('str').str.slice(0,10)
            df['Num_Serviço'] = df.Num_Serviço.astype('category')
            df.loc[df.Unidade == 'kHz', 'Frequência'] = df.loc[df.Unidade == 'kHz', 'Frequência'].apply(lambda x: Decimal(x) / Decimal(1000))
            df.loc[df.Unidade == 'GHz', 'Frequência'] = df.loc[df.Unidade == 'GHz', 'Frequência'].apply(lambda x: Decimal(x) * Decimal(1000))
            df['Frequência'] = df.Frequência.astype('float')
            df.loc[df.Unidade == 'kHz', 'Unidade'] = 'MHz'
            df = df_optimize(df, exclude=['Frequência'])
            try:
                df.to_feather(f"{pasta}/stel.fth")
            except ArrowInvalid:
                Path(f"{pasta}/stel.fth").unlink()
                df.to_excel(f"{pasta}/stel.xlsx", engine='openpyxl', index=False)
        except pyodbc.OperationalError:
            status.console.log(
                "Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!"
            )


def update_mosaico(pasta):
    """Atualiza a tabela local do Mosaico. É baixado e processado arquivos xml zipados da página pública do Spectrum E"""
    with console.status(
        "[blue]Baixando as Estações do Mosaico...", spinner="shark"
    ) as status:
        file = requests.get(ESTACOES)
        with open(f"{pasta}/estações.zip", "wb") as estações:
            estações.write(file.content)
    with console.status(
        "[blue]Baixando o Plano Básico das Estações...", spinner="weather"
    ) as status:
        file = requests.get(PLANO_BASICO)
        with open(f"{pasta}/Canais.zip", "wb") as plano_basico:
            plano_basico.write(file.content)
    console.print(":package: [blue]Consolidando as bases de dados...")
    estações = read_estações(f"{pasta}/estações.zip")
    plano_basico = read_plano_basico(f"{pasta}/Canais.zip")
    df = estações.merge(plano_basico, on="Id", how="left")
    df['Número_da_Estação'] = df['Número_da_Estação'].fillna(-1)
    df['Número_da_Estação'] = df['Número_da_Estação'].astype('int')
    df = clean_merge(pasta, df)
    try:
        df.reset_index(drop=True).to_feather(f"{pasta}/mosaico.fth")
    except ArrowInvalid:
        Path(f"{pasta}/mosaico.fth").unlink()
        with pd.ExcelWriter(f"{pasta}/mosaico.xlsx") as workbook:
            df.reset_index(drop=True).to_excel(workbook, sheet_name='Sheet1', engine="openpyxl", index=False)
    console.print("Kbô :vampire:")
    return df


def update_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False):
    """Wrapper que atualiza opcionalmente lê e atualiza as três bases indicadas anteriormente, as combina e salva o arquivo consolidado na pasta `pasta`"""
    stel = read_stel(pasta, up_stel).loc[:, TELECOM]
    radcom = read_radcom(pasta, up_radcom)
    radcom.rename(columns={"Número da Estação": "Número_da_Estação"}, inplace=True)
    mosaico = read_mosaico(pasta, up_mosaico)
    radcom["Num_Serviço"] = '231'
    radcom["Status"] = "RADCOM"
    radcom['Classe_Emissão'] = ''
    radcom['Largura_Emissão'] = ''
    radcom["Classe"] = radcom.Fase.str.strip() + "-" + radcom.Situação.str.strip()
    radcom["Entidade"] = radcom.Entidade.str.rstrip().str.lstrip()
    radcom["Num_Ato"] = '-1'
    radcom["Data_Ato"] = ""
    radcom["Validade_RF"] = ""
    radcom = radcom.loc[:, RADIODIFUSAO]
    radcom = df_optimize(radcom, exclude=['Frequência'])
    stel['Status'] = 'L'
    stel["Num_Ato"] = '-1'
    stel["Data_Ato"] = ""
    stel['Entidade'] = stel.Entidade.str.rstrip().str.lstrip()
    stel = df_optimize(stel, exclude=['Frequência'])
    mosaico = mosaico.loc[:, RADIODIFUSAO]
    mosaico['Classe_Emissão'] = ''
    mosaico['Largura_Emissão'] = ''
    mosaico = df_optimize(mosaico, exclude=['Frequência'])
    rd = mosaico.append(radcom)
    rd = rd.append(stel).sort_values("Frequência").reset_index(drop=True)
    rd['Num_Serviço'] = rd.Num_Serviço.astype('int')
    rd = df_optimize(rd, exclude=['Frequência'])
    console.print(":trophy: [green]Base Consolidada. Salvando os arquivos...")
    try:
        rd.to_feather(f"{pasta}/base.fth")
    except ArrowInvalid:
        Path(f"{pasta}/base.fth").unlink()
        with pd.ExcelWriter(f"{pasta}/base.xlsx") as workbook:
            rd.to_excel(workbook, sheet_name='Sheet1', engine="openpyxl", index=False)
    return rd


# Cell
def read_stel(pasta, update=False):
    """Lê o banco de dados salvo localmente do STEL. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou não exista o arquivo local"""
    if update:
        update_stel(pasta)
    file = Path(f"{pasta}/stel.fth")
    try:
        stel = pd.read_feather(file)
    except (ArrowInvalid, FileNotFoundError):
        file = Path(f"{pasta}/stel.xlsx")
        try:
            stel = pd.read_excel(file, engine="openpyxl")
        except FileNotFoundError:
            read_stel(pasta, True)

    return stel


def read_radcom(pasta, update=False):
    """Lê o banco de dados salvo localmente de RADCOM. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou não exista o arquivo local"""
    if update:
        update_radcom(pasta)
    file = Path(f"{pasta}/radcom.fth")
    try:
        radcom = pd.read_feather(file)
    except (ArrowInvalid, FileNotFoundError):
        file = Path(f"{pasta}/radcom.xlsx")
        try:
            radcom = pd.read_excel(file, engine="openpyxl")
        except FileNotFoundError:
            read_radcom(pasta, True)
    return radcom


def read_mosaico(pasta, update=False):
    """Lê o banco de dados salvo localmente do MOSAICO. Opcionalmente o atualiza antes da leitura baixando os diversos arquivos disponíveis na interface web pública"""
    if update:
        update_mosaico(pasta)
    file = Path(f"{pasta}/mosaico.fth")
    try:
        df = pd.read_feather(file)
    except (ArrowInvalid, FileNotFoundError):
        file = Path(f"{pasta}/mosaico.xlsx")
        try:
            df = pd.read_excel(file)
        except FileNotFoundError:
            return read_mosaico(pasta, update=True)
    return df_optimize(df, exclude=['Frequência'])


def read_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False):
    """Wrapper que combina a chamada das três funções de leitura do banco e opcionalmente é possível atualizá-las antes da leitura"""
    if any([up_stel, up_radcom, up_mosaico]):
        update_base(pasta, up_stel, up_radcom, up_mosaico)
    file = Path(f"{pasta}/base.fth")
    try:
        df =  pd.read_feather(file)
    except (ArrowInvalid, FileNotFoundError):
        file = Path(f"{pasta}/base.xlsx")
        try:
            df = pd.read_excel(file, engine='openpyxl')
        except FileNotFoundError:
            df = update_base(pasta, True, True, True)
    return df_optimize(df, exclude=['Frequência'])