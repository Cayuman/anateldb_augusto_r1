{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#default_exp parser\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import re\n",
    "import xml.etree.ElementTree as et\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pandas_read_xml as pdx\n",
    "from anateldb.query import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_stel(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente do STEL. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01.\"\"\"\n",
    "    if update:\n",
    "        update_stel(pasta)\n",
    "    if (file := Path(f'{pasta}/stel.fth')).exists():\n",
    "        stel = pd.read_feather(file)\n",
    "    elif (file := Path(f'{pasta}/stel.csv')).exists():\n",
    "        stel = pd.read_csv(file)\n",
    "    elif (file := Path(f'{pasta}/Base_de_Dados.xlsx')).exists():\n",
    "        stel = pd.read_excel(file, sheet_name='Stel', engine='openpyxl')\n",
    "    else:\n",
    "        update_stel(pasta)\n",
    "        try:\n",
    "            stel = pd.read_feather(Path(f'{pasta}/stel.fth'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise ConnectionError(\"Base de Dados do Stel inexistente e não foi possível atualizá-la\" ) from e\n",
    "    return stel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_radcom(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente de RADCOM. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01.\"\"\"\n",
    "    if update:\n",
    "        update_radcom(pasta)\n",
    "    if (file := Path(f'{pasta}/radcom.fth')).exists():\n",
    "        radcom = pd.read_feather(file)\n",
    "    elif (file := Path(f'{pasta}/radcom.csv')).exists():\n",
    "        radcom = pd.read_csv(file)\n",
    "    elif (file := Path(f'{pasta}/Base_de_Dados.xlsx')).exists():\n",
    "         radcom = pd.read_excel(file, sheet_name='Radcom', engine='openpyxl')\n",
    "    else:\n",
    "        update_radcom(pasta)\n",
    "        try:\n",
    "            radcom = pd.read_feather(Path(f'{pasta}/radcom.fth'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise ConnectionError(\"Base de Dados do Stel inexistente e não foi possível atualizá-la\" ) from e\n",
    "    return radcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_estações(row, cols=COL_ESTACOES):\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    dict_result = {k: row[k] for k in cols}\n",
    "    dict_result['Número da Estação'] = row['administrativo']['@numero_estacao']\n",
    "    return dict_result\n",
    "\n",
    "def parse_plano_basico(row, cols=COL_PLANO_BASICO):\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    if '@Frequencia' not in row:\n",
    "        cols = list(cols)\n",
    "        cols[cols.index('@Frequencia')] = '@Frequência'\n",
    "    return {k: row[k] for k in cols}\n",
    "\n",
    "def read_estações(path, cols):\n",
    "    df = pdx.read_xml(path, cols)\n",
    "    columns = ['Serviço', 'Situação', 'Entidade', 'Fistel', 'CNPJ', 'Município', 'UF', 'Id', 'Número da Estação']\n",
    "    dtypes = ['category', 'category', 'str', 'str', 'str', 'str', 'category', 'str', 'str']\n",
    "    df = pd.DataFrame(df['row'].apply(parse_estações).tolist())\n",
    "    df.columns = columns\n",
    "    for col, dtype in zip(columns, dtypes):\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def read_plano_basico(path, cols):\n",
    "    df = pdx.read_xml(path, cols)\n",
    "    columns = ['Id', 'Classe', 'Frequência', 'Latitude', 'Longitude']\n",
    "    dtypes = ['str', 'category', 'str', 'str', 'str']\n",
    "    df = pd.DataFrame(df['row'].apply(parse_plano_basico).tolist())\n",
    "    df.loc[df['@Frequencia'].isna(), '@Frequencia'] = df.loc[df['@Frequência'].notna(), '@Frequência']\n",
    "    df.drop('@Frequência', axis=1, inplace=True)\n",
    "    df.columns = columns\n",
    "    for col, dtype in zip(columns, dtypes):\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def read_historico(path):\n",
    "    regex = r\"\\s([a-zA-Z]+)=\\'{1}([\\w\\-\\ :\\.]*)\\'{1}\"\n",
    "    with ZipFile(path) as xmlzip:\n",
    "        with xmlzip.open('documento_historicos.xml', 'r') as xml:\n",
    "            xml_list = xml.read().decode().split('\\n')[2:-1]\n",
    "    dict_list = []\n",
    "    for item in xml_list:\n",
    "        matches = re.finditer(regex, item, re.MULTILINE)\n",
    "        dict_list.append(dict(match.groups() for match in matches))\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df = df[(df.tipoDocumento == 'Ato') & (df.razao == 'Autoriza o Uso de Radiofrequência')].reset_index()\n",
    "    df = df.loc[:, ['id', 'numeroDocumento', 'orgao', 'dataDocumento']]\n",
    "    df.columns = ['Id', 'Num_Ato', 'Órgao', 'Data_Ato']\n",
    "    df['Data_Ato'] = pd.to_datetime(df.Data_Ato)    \n",
    "    return df.sort_values('Data_Ato').groupby('Id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_mosaico(pasta, update=False):\n",
    "    if update:\n",
    "        update_mosaico(pasta)\n",
    "        estações = read_estações(f'{pasta}/estações.zip', ['estacao_rd'])\n",
    "        plano_basico = read_plano_basico(f'{pasta}/plano_basico.zip', ['plano_basico'])\n",
    "        historico = read_historico(f'{pasta}/historico.zip')\n",
    "        df = pd.merge(estações, plano_basico, on='Id').merge(historico, on='Id')\n",
    "        df.to_feather(f'{pasta}/mosaico.fth')\n",
    "        return df\n",
    "    if not (file := Path(f'{pasta}/mosaico.fth')).exists():\n",
    "            return read_mosaico(pasta, update=True)\n",
    "    return pd.read_feather(f'{pasta}/mosaico.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted filter.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted parser.ipynb.\n",
      "Converted queries.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anateldb]",
   "language": "python",
   "name": "conda-env-anateldb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
