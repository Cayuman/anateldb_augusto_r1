{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#default_exp parser\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "> Funções principais de Leitura e Processamento dos dados provenientes dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import re\n",
    "import xml.etree.ElementTree as et\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pandas_read_xml as pdx\n",
    "from anateldb.query import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_stel(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente do STEL. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01.\"\"\"\n",
    "    if update:\n",
    "        update_stel(pasta)\n",
    "    if (file := Path(f'{pasta}/stel.fth')).exists():\n",
    "        stel = pd.read_feather(file)\n",
    "    elif (file := Path(f'{pasta}/stel.csv')).exists():\n",
    "        stel = pd.read_csv(file)\n",
    "    elif (file := Path(f'{pasta}/Base_de_Dados.xlsx')).exists():\n",
    "        stel = pd.read_excel(file, sheet_name='Stel', engine='openpyxl')\n",
    "    else:\n",
    "        update_stel(pasta)\n",
    "        try:\n",
    "            stel = pd.read_feather(Path(f'{pasta}/stel.fth'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise ConnectionError(\"Base de Dados do Stel inexistente e não foi possível atualizá-la\" ) from e\n",
    "    return stel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_radcom(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente de RADCOM. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01.\"\"\"\n",
    "    if update:\n",
    "        update_radcom(pasta)\n",
    "    if (file := Path(f'{pasta}/radcom.fth')).exists():\n",
    "        radcom = pd.read_feather(file)\n",
    "    elif (file := Path(f'{pasta}/radcom.csv')).exists():\n",
    "        radcom = pd.read_csv(file)\n",
    "    elif (file := Path(f'{pasta}/Base_de_Dados.xlsx')).exists():\n",
    "         radcom = pd.read_excel(file, sheet_name='Radcom', engine='openpyxl')\n",
    "    else:\n",
    "        update_radcom(pasta)\n",
    "        try:\n",
    "            radcom = pd.read_feather(Path(f'{pasta}/radcom.fth'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise ConnectionError(\"Base de Dados do Stel inexistente e não foi possível atualizá-la\" ) from e\n",
    "    return radcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def row2dict(row):\n",
    "    \"\"\"Receives a json row and return the dictionary from it\"\"\"\n",
    "    #dict_result = {k: row[k] for k in cols}\n",
    "    #dict_result['Número da Estação'] = row['administrativo']['@numero_estacao']\n",
    "    return {k:v for k,v in row.items()}\n",
    "\n",
    "def dict2cols(df, reject=()):\n",
    "    \"\"\"Recebe um dataframe com dicionários nas células e extrai os dicionários como colunas\n",
    "       Opcionalmente ignora e exclue as colunas em reject\n",
    "       \"\"\"    \n",
    "    for column in df.columns:\n",
    "        if column in reject:\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "            continue\n",
    "        if type(df[column].iloc[0]) == collections.OrderedDict:\n",
    "            try:\n",
    "                new_df = pd.DataFrame(df[column].apply(row2dict).tolist())\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "def parse_plano_basico(row, cols=COL_PB):\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    return {k: row[k] for k in cols}\n",
    "\n",
    "def read_estações(path):\n",
    "    df = pdx.read_xml(path, ['estacao_rd'])\n",
    "    df = pd.DataFrame(df['row'].apply(row2dict).tolist()).replace('', pd.NA)\n",
    "    df = dict2cols(df)\n",
    "    df = df.loc[:, COL_ESTACOES]\n",
    "    df.columns = NEW_ESTACOES\n",
    "    return df\n",
    "    #df['Número da Estação'] = row['administrativo']['@numero_estacao']\n",
    "\n",
    "def read_plano_basico(path):\n",
    "    df = pdx.read_xml(path, ['plano_basico'])\n",
    "    df = pd.DataFrame(df['row'].apply(row2dict).tolist()).replace('', pd.NA)\n",
    "    df = dict2cols(df, REJECT_ESTACOES)\n",
    "    df.loc[df['@Frequência'].isna(), '@Frequência'] = df.loc[df['@Frequência'].isna(), '@Frequencia']\n",
    "    df.drop('@Frequencia', axis=1, inplace=True)\n",
    "    df.loc[df['@País'].isna(), '@País'] = df.loc[df['@País'].isna(), '@Pais']\n",
    "    df.drop('@Pais', axis=1, inplace=True)\n",
    "    df = df.loc[:, COL_PB]\n",
    "    df.columns = NEW_PB\n",
    "    return df\n",
    "#     df['@Longitude'] = df['@Longitude'].str.replace(',', '.').astype('float')\n",
    "#     df['@Latitude'] = df['@Latitude'].str.replace(',', '.').astype('float')\n",
    "#     df['@Frequência'] = df['@Frequência'].str.replace(',', '.').astype('float')\n",
    "    \n",
    "\n",
    "def read_historico(path):\n",
    "    regex = r\"\\s([a-zA-Z]+)=\\'{1}([\\w\\-\\ :\\.]*)\\'{1}\"\n",
    "    with ZipFile(path) as xmlzip:\n",
    "        with xmlzip.open('documento_historicos.xml', 'r') as xml:\n",
    "            xml_list = xml.read().decode().split('\\n')[2:-1]\n",
    "    dict_list = []\n",
    "    for item in xml_list:\n",
    "        matches = re.finditer(regex, item, re.MULTILINE)\n",
    "        dict_list.append(dict(match.groups() for match in matches))\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df = df[(df.tipoDocumento == 'Ato') & (df.razao == 'Autoriza o Uso de Radiofrequência')].reset_index()\n",
    "    df = df.loc[:, ['id', 'numeroDocumento', 'orgao', 'dataDocumento']]\n",
    "    df.columns = ['Id', 'Num_Ato', 'Órgao', 'Data_Ato']\n",
    "    df['Data_Ato'] = pd.to_datetime(df.Data_Ato)    \n",
    "    return df.sort_values('Data_Ato').groupby('Id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_mosaico(pasta, update=False):\n",
    "    if update:\n",
    "        update_mosaico(pasta)\n",
    "        estações = read_estações(f'{pasta}/estações.zip', ['estacao_rd'])\n",
    "        plano_basico = read_plano_basico(f'{pasta}/plano_basico.zip', ['plano_basico'])\n",
    "        historico = read_historico(f'{pasta}/historico.zip')\n",
    "        df = pd.merge(estações, plano_basico, on='Id').merge(historico, on='Id')\n",
    "        df.to_feather(f'{pasta}/mosaico.fth')\n",
    "        return df\n",
    "    if not (file := Path(f'{pasta}/mosaico.fth')).exists():\n",
    "            return read_mosaico(pasta, update=True)\n",
    "    return pd.read_feather(f'{pasta}/mosaico.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted eda.ipynb.\n",
      "Converted filter.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted parser.ipynb.\n",
      "Converted queries.ipynb.\n",
      "Converted tests.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anateldb]",
   "language": "python",
   "name": "conda-env-anateldb-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
