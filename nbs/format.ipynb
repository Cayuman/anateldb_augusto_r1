{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp format\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from typing import List, Iterable, Union\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from decimal import Decimal\n",
    "\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from gazpacho import Soup\n",
    "import requests\n",
    "from fastcore.utils import listify\n",
    "from rich.progress import track\n",
    "\n",
    "from anateldb.constants import ENTIDADES, COL_PB, ESTACAO, BW, BW_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatação \n",
    "\n",
    "> Este módulo possui funções auxiliares de formatação dos dados das várias fontes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def row2dict(row: dict) -> dict:  # sourcery skip: identity-comprehension\n",
    "    \"\"\"Receives a json row and return the dictionary from it\"\"\"\n",
    "    return dict(row.items())\n",
    "\n",
    "\n",
    "def dict2cols(df: pd.DataFrame, reject: Iterable[str] = ()) -> pd.DataFrame:\n",
    "    \"\"\"Recebe um dataframe com dicionários nas células e extrai os dicionários como colunas\n",
    "    Opcionalmente ignora e exclue as colunas em reject\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if column in reject:\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "            continue\n",
    "        if type(df[column].iloc[0]) == OrderedDict:\n",
    "            try:\n",
    "                new_df = pd.DataFrame(df[column].apply(row2dict).tolist())\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_plano_basico(row: dict, cols: Iterable[str] = COL_PB) -> dict:\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    return {k: row[k] for k in cols}\n",
    "\n",
    "\n",
    "def scrape_dataframe(id_list: Iterable[str]) -> pd.DataFrame:\n",
    "    \"\"\"Receives a list of ids and returns a dataframe with the data from web scraping the MOSAICO page\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for id_ in track(id_list, description=\"Baixando informações complementares da Web\"):\n",
    "        html = requests.get(ESTACAO.format(id_))\n",
    "        df = df.append(\n",
    "            pd.read_html(Soup(getattr(html, \"text\", \"\")).find(\"table\").html)[0]\n",
    "        )\n",
    "\n",
    "    df.rename(\n",
    "        columns={\"NumFistel\": \"Fistel\", \"Num Serviço\": \"Num_Serviço\"}, inplace=True\n",
    "    )\n",
    "    return df[\n",
    "        [\n",
    "            \"Status\",\n",
    "            \"Entidade\",\n",
    "            \"Fistel\",\n",
    "            \"Frequência\",\n",
    "            \"Classe\",\n",
    "            \"Num_Serviço\",\n",
    "            \"Município\",\n",
    "            \"UF\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def input_coordenates(df: pd.DataFrame, pasta: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Input the NA's in Coordinates with those of the cities\"\"\"\n",
    "    municipios = Path(f\"{pasta}/municípios.fth\")\n",
    "    if not municipios.exists():\n",
    "        municipios = Path(f\"{pasta}/municípios.xlsx\")\n",
    "        if not municipios.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"É necessario a tabela de municípios municípios.fth | municípios.xlsx na pasta {pasta}\"\n",
    "            )\n",
    "        m = pd.read_excel(municipios, engine=\"openpyxl\")\n",
    "    else:\n",
    "        m = pd.read_feather(municipios)\n",
    "    m.loc[\n",
    "        m.Município == \"Sant'Ana do Livramento\", \"Município\"\n",
    "    ] = \"Santana do Livramento\"\n",
    "    m[\"Município\"] = m.Município.apply(unidecode).str.lower().str.replace(\"'\", \" \")\n",
    "    m[\"UF\"] = m.UF.str.lower()\n",
    "    df[\"Coordenadas_do_Município\"] = False\n",
    "    df[\"Latitude\"] = df.Latitude.str.replace(\",\", \".\")\n",
    "    df[\"Longitude\"] = df.Longitude.str.replace(\",\", \".\")\n",
    "    df.loc[df[\"Município\"] == \"Poxoréo\", \"Município\"] = \"Poxoréu\"\n",
    "    df.loc[df[\"Município\"] == \"Couto de Magalhães\", \"Município\"] = \"Couto Magalhães\"\n",
    "    df[\"Município\"] = df.Município.astype(\"string\")\n",
    "    criteria = (\n",
    "        (df.Latitude == \"\")\n",
    "        | (df.Latitude.isna())\n",
    "        | (df.Longitude == \"\")\n",
    "        | (df.Longitude.isna())\n",
    "    ) & df.Município.isna()\n",
    "    df = df[~criteria]\n",
    "    for row in df[\n",
    "        (\n",
    "            (df.Latitude == \"\")\n",
    "            | (df.Latitude.isna())\n",
    "            | (df.Longitude == \"\")\n",
    "            | (df.Longitude.isna())\n",
    "        )\n",
    "    ].itertuples():\n",
    "        try:\n",
    "            left = unidecode(row.Município).lower()\n",
    "            m_coord = (\n",
    "                m.loc[\n",
    "                    (m.Município == left) & (m.UF == row.UF.lower()),\n",
    "                    [\"Latitude\", \"Longitude\"],\n",
    "                ]\n",
    "                .values.flatten()\n",
    "                .tolist()\n",
    "            )\n",
    "            df.loc[row.Index, \"Latitude\"] = m_coord[0]\n",
    "            df.loc[row.Index, \"Longitude\"] = m_coord[1]\n",
    "            df.loc[row.Index, \"Coordenadas_do_Município\"] = True\n",
    "        except ValueError:\n",
    "            print(left, row.UF, m_coord)\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_bw(bw: str) -> float:\n",
    "    \"\"\"Parse the bandwidth string\"\"\"\n",
    "    if match := re.match(BW_pattern, bw):\n",
    "        multiplier = BW[match.group(2)]\n",
    "        if mantissa := match.group(3):\n",
    "            number = float(f\"{match.group(1)}.{mantissa}\")\n",
    "        else:\n",
    "            number = float(match.group(1))\n",
    "        return multiplier * number\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização dos Tipos de dados\n",
    "A serem criados dataframes, normalmente a tipo de data é aquele com maior resolução possível, nem sempre isso é necessário, os arquivos de espectro mesmo possuem somente uma casa decimal, portanto um `float16` já é suficiente para armazená-los. As funções a seguir fazem essa otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below borrowed from https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def optimize_floats(df: pd.DataFrame, exclude: Iterable[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Optimize the floats in the dataframe to reduce the memory usage\"\"\"\n",
    "    floats = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "    floats = [c for c in floats if c not in listify(exclude)]\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame, exclude: Iterable[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Optimize the ints in the dataframe to reduce the memory usage\"\"\"\n",
    "    ints = df.select_dtypes(include=[\"int64\"]).columns.tolist()\n",
    "    ints = [c for c in ints if c not in listify(exclude)]\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_objects(\n",
    "    df: pd.DataFrame,\n",
    "    datetime_features: Iterable[str] = None,\n",
    "    exclude: Iterable[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Optimize the objects in the dataframe to category | string to reduce the memory usage\"\"\"\n",
    "    exclude = listify(exclude)\n",
    "    datetime_features = listify(datetime_features)\n",
    "    for col in df.select_dtypes(\n",
    "        include=[\"object\", \"string\", \"category\"]\n",
    "    ).columns.tolist():\n",
    "        if col not in datetime_features:\n",
    "            if col in exclude:\n",
    "                continue\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                dtype = \"category\"\n",
    "            else:\n",
    "                dtype = \"string\"\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col]).dt.date\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_optimize(\n",
    "    df: pd.DataFrame,\n",
    "    datetime_features: Iterable[str] = None,\n",
    "    exclude: Iterable[str] = None,\n",
    "):\n",
    "    \"\"\"Optimize the data types in dataframe to reduce the memory usage\"\"\"\n",
    "    if datetime_features is None:\n",
    "        datetime_features = []\n",
    "    return optimize_floats(\n",
    "        optimize_ints(optimize_objects(df, datetime_features, exclude), exclude),\n",
    "        exclude,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Apresentação REFIS 2022 - anateldb.ipynb.\n",
      "Converted constants.ipynb.\n",
      "Converted filter.ipynb.\n",
      "Converted format.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted merge.ipynb.\n",
      "Converted query.ipynb.\n",
      "Converted read.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "737b3e0e8750f89ce31674571febe5a5a08e902b32bb8cbee589bdf91ca77e61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
